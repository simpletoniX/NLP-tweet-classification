{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b70c6876aad65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.046419Z",
     "start_time": "2024-03-07T15:19:20.731484Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.085393Z",
     "start_time": "2024-03-07T15:19:22.392976Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>username</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words , your food was crapilicious!</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is  so white?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a classy whore? Or more red velvet cupcakes?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meh. :P  thanks for the heads up, but not too...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is an ISIS account pretending to be a Ku...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0      In other words , your food was crapilicious!   not_cyberbullying   \n",
       "1                         Why is  so white?           not_cyberbullying   \n",
       "2       a classy whore? Or more red velvet cupcakes?  not_cyberbullying   \n",
       "3   meh. :P  thanks for the heads up, but not too...  not_cyberbullying   \n",
       "4   This is an ISIS account pretending to be a Ku...  not_cyberbullying   \n",
       "\n",
       "   username  tag  \n",
       "0         0    2  \n",
       "1         0   10  \n",
       "2         1    0  \n",
       "3         1    0  \n",
       "4         1    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/tonix/PycharmProjects/pythonProject/Cyberbulling/csv/cyberbullying_tweets2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6103f5c79326be0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.087694Z",
     "start_time": "2024-03-07T15:19:22.483124Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6681fcaefbf9f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.096190Z",
     "start_time": "2024-03-07T15:19:22.497569Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd43735072e7aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.098406Z",
     "start_time": "2024-03-07T15:19:22.506557Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text            7\n",
       "cyberbullying_type    0\n",
       "username              0\n",
       "tag                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9b3092220bbe1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.112860Z",
     "start_time": "2024-03-07T15:19:22.510438Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'not_cyberbullying' 0 1]\n",
      "[nan 'not_cyberbullying' 1 0]\n",
      "[nan 'gender' 0 1]\n",
      "[nan 'gender' 1 0]\n",
      "[nan 'other_cyberbullying' 1 0]\n",
      "[nan 'other_cyberbullying' 0 1]\n",
      "[nan 'ethnicity' 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's check nan values\n",
    "null_values = df[df.isnull().any(axis=1)]\n",
    "\n",
    "for i in null_values.values:\n",
    "    print(i)\n",
    "\n",
    "# Okay, I will leave it here cause we have another features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ef64605c37c332",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# >>> Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f75f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete links \n",
    "template = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "df['tweet_text'] = df['tweet_text'].apply(lambda x: template.sub(\"\", x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e77e3c8ae73f679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.175756Z",
     "start_time": "2024-03-07T15:19:23.037065Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Delete emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"  \n",
    "                           u\"\\U000024C2-\\U0001F251\"  \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "df['tweet_text'] = df['tweet_text'].apply(lambda text: emoji_pattern.sub(r\"\", text) if isinstance(text, str) else text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f32d81fbe58672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.176428Z",
     "start_time": "2024-03-07T15:19:23.178803Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Delete every letters which repeat 3 or more times\n",
    "df['tweet_text'] = df['tweet_text'].apply(lambda text: re.sub(r'(\\w)\\1{2,}', r'\\1', str(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e49ce4b4859aebce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.176675Z",
     "start_time": "2024-03-07T15:19:23.333211Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for delete numbers \n",
    "def delete_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function for delete special symbols\n",
    "def delete_symbols(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Function for lower text\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function for delete \"RT\"\n",
    "def delete_rt(text):\n",
    "    return re.sub(r'\\brt\\b|\\bRT\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc3135bde2fdd67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:24.195178Z",
     "start_time": "2024-03-07T15:19:23.396132Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply functions to df \n",
    "df['tweet_text'] = df['tweet_text'].apply(lambda text: lower_text(delete_rt(delete_symbols(delete_numbers(text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56174b170854bc20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.766466Z",
     "start_time": "2024-03-07T15:19:23.586766Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create tokens \n",
    "df['tweet_text'] = df['tweet_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ce2ae5d7c9f5ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.839912Z",
     "start_time": "2024-03-07T15:19:26.083690Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for creating tokens and delete stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e873acb8cfd00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.852023Z",
     "start_time": "2024-03-07T15:19:26.098393Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply function of stop_words\n",
    "df['tweet_text'] = df['tweet_text'].apply(lambda text: remove_stop_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa2c74306301f98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.870895Z",
     "start_time": "2024-03-07T15:19:26.141222Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>username</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12024</th>\n",
       "      <td>[girls, bringing, sassyges, table]</td>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40949</th>\n",
       "      <td>[cant, believe, mtv, actually, show, bunch, bl...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42157</th>\n",
       "      <td>[mad, aw, fuck, obama, dumb, ass, nigger]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9256</th>\n",
       "      <td>[think, guy, mentally, fucking, retarded]</td>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35587</th>\n",
       "      <td>[stop, preying, sorority, sisters, girls, bull...</td>\n",
       "      <td>age</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type  \\\n",
       "12024                 [girls, bringing, sassyges, table]             gender   \n",
       "40949  [cant, believe, mtv, actually, show, bunch, bl...          ethnicity   \n",
       "42157          [mad, aw, fuck, obama, dumb, ass, nigger]          ethnicity   \n",
       "9256           [think, guy, mentally, fucking, retarded]             gender   \n",
       "35587  [stop, preying, sorority, sisters, girls, bull...                age   \n",
       "\n",
       "       username  tag  \n",
       "12024         1    1  \n",
       "40949         0    1  \n",
       "42157         2    0  \n",
       "9256          1    0  \n",
       "35587         0    0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd37d69d1f0bf6b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.880160Z",
     "start_time": "2024-03-07T15:19:26.147495Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610992"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Searching word changers | count words\n",
    "word_list = []\n",
    "word_count = [] \n",
    "\n",
    "for list in df['tweet_text']:\n",
    "    for word in list:\n",
    "        \n",
    "        # Append word changers\n",
    "        if 1 < len(word) < 3: \n",
    "            word_list.append(word)\n",
    "        \n",
    "        # Count words\n",
    "        word_count.append(word)\n",
    "        \n",
    "word_list_keys = Counter(word_list)\n",
    "len(word_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b9606d27524e3d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.881084Z",
     "start_time": "2024-03-07T15:19:26.198983Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20663"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word values sum\n",
    "sum_keys = sum(word_list_keys.values())\n",
    "sum_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa43265d01596cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.881823Z",
     "start_time": "2024-03-07T15:19:26.199440Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3861373229681058%\n"
     ]
    }
   ],
   "source": [
    "# As we can see we have something like 3% word changer in our dataset, I don't have any other options now except to leave it \n",
    "sum_in_percents = (20777 / 613590) * 100\n",
    "print(f\"{sum_in_percents}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "332f045a2689efd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.882208Z",
     "start_time": "2024-03-07T15:19:26.199630Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# >>> Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98f9fbe093b0f186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.882365Z",
     "start_time": "2024-03-07T15:19:26.199846Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for creating tuples (word, tag)\n",
    "def tuple_convertor(word_list):\n",
    "    # Filtering of words\n",
    "    tagged_words = nltk.pos_tag(word_list)\n",
    "    \n",
    "    return tagged_words        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ab38f61ddd0332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.882510Z",
     "start_time": "2024-03-07T15:19:26.204538Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for transforming tag\n",
    "def get_wordnet_pos(word_tag_list):\n",
    "    # transforming pos-tags\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "           \"V\": wordnet.VERB,\n",
    "           \"N\": wordnet.NOUN,\n",
    "           \"R\": wordnet.ADV}\n",
    "    \n",
    "    wordnet_tags = [tag_dict.get(tag[0].upper(), wordnet.NOUN) for word, tag in word_tag_list]\n",
    "    return wordnet_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e4c75dccc62ea19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:26.882638Z",
     "start_time": "2024-03-07T15:19:26.204790Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for lemmatizaiton of words\n",
    "def lemmatization_words(tokens_list):\n",
    "    # >>> lematizing of words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tuple_list = tuple_convertor(tokens_list)\n",
    "    wordnet_tags = get_wordnet_pos(tuple_list)\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, tag) for (word, _), tag in zip(tuple_list, wordnet_tags)]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a1200421bc00672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:27.485693Z",
     "start_time": "2024-03-07T15:19:26.205043Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_list = [\"The\", \"quick\", \"brown\", \"foxes\"]\n",
    "lemmatization_words(example_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "956b85450445646e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:42.514809Z",
     "start_time": "2024-03-07T15:19:27.007084Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Applying filtered_words_by_tag to input data\n",
    "df['tweet_text'] = df['tweet_text'].apply(lemmatization_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8054ab70bc8feb72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:42.540650Z",
     "start_time": "2024-03-07T15:19:42.491900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>username</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[word, food, crapilicious]</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[white]</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[classy, whore, red, velvet, cupcake]</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[meh, p, thanks, head, concern, another, angry...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[isi, account, pretend, kurdish, account, like...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0                         [word, food, crapilicious]  not_cyberbullying   \n",
       "1                                            [white]  not_cyberbullying   \n",
       "2              [classy, whore, red, velvet, cupcake]  not_cyberbullying   \n",
       "3  [meh, p, thanks, head, concern, another, angry...  not_cyberbullying   \n",
       "4  [isi, account, pretend, kurdish, account, like...  not_cyberbullying   \n",
       "\n",
       "   username  tag  \n",
       "0         0    2  \n",
       "1         0   10  \n",
       "2         1    0  \n",
       "3         1    0  \n",
       "4         1    0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7487b9a07e14687c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:42.543213Z",
     "start_time": "2024-03-07T15:19:42.508873Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age',\n",
       " 'ethnicity',\n",
       " 'gender',\n",
       " 'not_cyberbullying',\n",
       " 'other_cyberbullying',\n",
       " 'religion'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['cyberbullying_type'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28c32648c074d15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:42.543435Z",
     "start_time": "2024-03-07T15:19:42.509620Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# >>> Most common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d62aa1feba5dd2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:47.810521Z",
     "start_time": "2024-03-07T15:19:42.509966Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# New df\n",
    "age_bullying = df[df['cyberbullying_type'] == 'age']\n",
    "gender_bullying = df[df['cyberbullying_type'] == 'gender']\n",
    "religion = df[df['cyberbullying_type'] == 'religion']\n",
    "not_bullying = df[df['cyberbullying_type'] == 'not_cyberbullying']\n",
    "\n",
    "# Create lists with bullying types\n",
    "all_age_words = sum(age_bullying['tweet_text'].tolist(), [])\n",
    "all_gender_bullying = sum(gender_bullying['tweet_text'].tolist(), [])\n",
    "all_religion_bullying = sum(religion['tweet_text'].tolist(), [])\n",
    "all_not_bullying = sum(not_bullying['tweet_text'].tolist(), [])\n",
    "\n",
    "# Word counts | Top 200 words | Avg words\n",
    "age_words_count = Counter(all_age_words).most_common()\n",
    "gender_words_count = Counter(all_gender_bullying).most_common()\n",
    "religion_words_count = Counter(all_religion_bullying).most_common()\n",
    "not_bullying_count = Counter(all_not_bullying).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28156126-54b9-45ba-85fb-ec29cc881939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean lenght: 12.860011365788974\n",
      "outliers: 894\n"
     ]
    }
   ],
   "source": [
    "df['vector_length'] = df['tweet_text'].apply(len)\n",
    "\n",
    "mean_length = df['vector_length'].mean()\n",
    "outliers = df[df['vector_length'] > 30]\n",
    "\n",
    "df = df[df['vector_length'] < 30]\n",
    "\n",
    "print(f\"mean lenght: {mean_length}\")\n",
    "print(f\"outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b75d9d0-6c3e-44c2-b248-0b63135f0644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[word, food, crapilicious]</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[white]</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[classy, whore, red, velvet, cupcake]</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[meh, p, thanks, head, concern, another, angry...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[isi, account, pretend, kurdish, account, like...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0                         [word, food, crapilicious]  not_cyberbullying\n",
       "1                                            [white]  not_cyberbullying\n",
       "2              [classy, whore, red, velvet, cupcake]  not_cyberbullying\n",
       "3  [meh, p, thanks, head, concern, another, angry...  not_cyberbullying\n",
       "4  [isi, account, pretend, kurdish, account, like...  not_cyberbullying"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "id": "6f7172ddaf9b0ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:48.154022Z",
     "start_time": "2024-03-07T15:19:48.150537Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# >>> Transformating it into vectors"
=======
   "execution_count": 31,
   "id": "756281a0-a0bf-4f68-b5ee-36fe8bbf7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "df.drop(['username', 'tag', 'vector_length'], axis=1, inplace=True)"
>>>>>>> 5d2e644 (Update)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "id": "ec48797afd752a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:48.385774Z",
     "start_time": "2024-03-07T15:19:48.384173Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Creating vectors\n",
    "vectors = []\n",
    "\n",
    "for list in tf_idf_scores:\n",
    "    vector = [val for val in list.values()]\n",
    "\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de55f8cc675ab102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:48.397216Z",
     "start_time": "2024-03-07T15:19:48.386392Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Change df column\n",
    "df['tweet_text'] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 32,
>>>>>>> 5d2e644 (Update)
   "id": "3f8af7a4ecc09e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:19:48.729251Z",
     "start_time": "2024-03-07T15:19:48.392161Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save csv\n",
    "df.to_csv('ready_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
